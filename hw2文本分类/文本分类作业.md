## 文本分类作业

**姓名：** 李婧如			**学号：** 1700012993

- 数据预处理
  - 停用词采用 `nltk.corpus` 中的 `stopwords` 列表
  
  - 分词首先用 `nltk.tokenize` 做第一步划分，接着使用 `wordninja` 将某些未用空格分开的词做划分（在另一种方法中未采用这一步骤，将不在 `glove` 词表中的所有词都划分为 `out of vocabulary`，并给这些词一个统一的 300 维词向量）
  
  - 采用 `glove` 词向量做 `embedding`，向量为 300 维。现在文本中找到出现过的词，在 `glove` 词表中找到对应的词向量，形成词典（词表中包含的存在 `dic` ，不包含的存在 `oov` ）。该部分对应 `readData.py` 中的 `dic(key = word_str, value = embedding vector)`  。
  
    ```python
    for item in line:
        wo = item.split(" ")[0]
        if wo in dic.keys():
            ve = item[:-1].split(" ")[1:]
            dic[wo] = ve
    
    for item in dic.keys():
        if dic[item] == 0:
            oov[item] = lt
    
    for item in oov.keys():
        dic.pop(item)
    ```
- 分类方法
  - `textCNN`，在数据预处理方面尝试了对连在一起的词进行划分和不划分两种；模型训练方面，尝试了应用 `extra.txt` 在模型预测出的结果和 `train.txt` 作为训练数据。
  
  - 模型结构，利用四层卷积提取特征，最后用全连接层进行分类。损失函数选用交叉熵损失。
  
    ```python
    class textCNN(nn.Module):
        def __init__(self,args):
            super(textCNN, self).__init__()
            vocb_size = args['vocb_size']
            dim = args['dim']
            n_class = args['n_class']
            max_len = args['max_len']
            embedding_matrix=args['embedding_matrix']
            #将事先训练好的词向量载入
            self.embeding = nn.Embedding(vocb_size, dim,_weight=embedding_matrix)
            self.conv1 = nn.Sequential(
                         nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5,
                                   stride=1, padding=2),
                         nn.ReLU(),
                         nn.MaxPool2d(kernel_size=2) # (16,64,64)
                         )
            self.conv2 = nn.Sequential(
                         nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=2, padding=2),
                         nn.ReLU(),
                         nn.MaxPool2d(2)
                         )
            self.conv3 = nn.Sequential(
                nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=2, padding=2),
                nn.ReLU(),
                nn.MaxPool2d(2)
            )
            self.conv4 = nn.Sequential(  # (16,64,64)
                nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=2, padding=2),
                nn.ReLU(),
                nn.MaxPool2d(2)
            )
            self.out = nn.Linear(512, n_class)
    ```
  
  - 部分模型参数
    - `batch_size = 100`
    - `epoch`
    - `lr = 0.005`
- `self_test` 上的评测结果
  - 采用 `wordninja` 处理后数据的结果，`lr = 0.0005` ，`epoch = 30`：
  - `micro-{precision, recall, F1} = (0.853, 0.853, 0.853)`
  - `macro-{precision, recall, F1} = (0.870, 0.843, 0.852)`
  - 未处理，`lr = 0.001` ，`epoch = 10`：
  - `micro-{precision, recall, F1} = (0.876, 0.882, 0.879)`
  - `macro-{precision, recall, F1} = (0.877, 0.877, 0.877)`
  - （最终选择该模型预测 `test.txt` ）
  - 采用 `extra.txt` 的预测结果作为数据的结果，`lr = 0.01` ，`epoch = 10`
  - `micro-{precision, recall, F1} = (0.865, 0.885, 0.874)`
  - `macro-{precision, recall, F1} = (0.872, 0.872, 0.872)`
- 实验结果分析
  - 不同方法的比较：
  - 预处理方面，未采用 `wordninja` 对 `oov` 做处理的结果优于用处理后的数据进行训练得到的结果。可能因为少量的词汇已经能得到较好的特征表示，加入更多的词汇并不会使结果更好（类似于停用词的原理）。
  - 对于采用 `extra.txt`预测结果和 `train.txt` 作为训练的组，其准确率较只使用 `train.txt` 并无较大提升。一部分原因可能是 `textCNN` 的能力有限，还有可能因为并没有对预测的标签进行有效的筛选，导致其含有大量的与预测结果同分布的数据，且含部分错误标注的数据。（这个方法参考了课上所讲的半监督学习，但遗憾的是并没有对筛选错误标注的数据做进一步研究）
- Reference
  - `wordninja` 分词参考： https://github.com/keredson/wordninja 
  - `textCNN` 参考：《Convolutional Neural Networks for Sentence Classification》 

